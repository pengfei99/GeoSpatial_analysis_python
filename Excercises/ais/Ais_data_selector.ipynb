{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40d13d5-3fc1-4832-ad2d-c28bb2941c1e",
   "metadata": {},
   "source": [
    "# Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6440a52d-4336-44a0-92e2-fd32fcadf1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "based on: https://github.com/datasciencecampus/UNGP-AIS-ETL/blob/feature/StructuredStreaming/spark_etl_bench/src/utils.py \n",
    "\n",
    "\"\"\"\n",
    "from ._poly import poly_container\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Set, Dict, List, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "import h3\n",
    "import h3.api.numpy_int as h3int\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from shapely.geometry import shape\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.types import StructField, StructType\n",
    "\n",
    "# from sedona.register import SedonaRegistrator\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s:%(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def apply_small_filter(\n",
    "    spark: SparkSession,\n",
    "    ais_df: DataFrame,\n",
    "    filter_criteria,\n",
    "    column_filter: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"A wrapper function on broadcast-join. This is efficient for small list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark: SparkSession\n",
    "    \n",
    "    ais_df: Spark DataFrame\n",
    "        The dataframe should contain 'column_filter' column\n",
    "    \n",
    "    filter_values: Pandas DataFrame or List\n",
    "        If List - list of values for the filter\n",
    "        If Pandas Dataframe, should contain 'column_filter' column. Other columns will be included\n",
    "        in the output.\n",
    "        \n",
    "    column_filter: str\n",
    "        name of column to apply the filter\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Spark dataframe with filter applied\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if column_filter not in ais_df.columns:\n",
    "        raise Exception(\"column_filter should be in ais_df columns\")\n",
    "    \n",
    "    if type(filter_criteria) is list:\n",
    "        matrix = [[i] for i in filter_criteria]\n",
    "        df_filter = spark.createDataFrame(matrix, schema=[column_filter])\n",
    "        \n",
    "    elif isinstance(filter_criteria, pd.DataFrame):\n",
    "        if column_filter in filter_criteria.columns:\n",
    "            df_filter = spark.createDataFrame(filter_criteria)\n",
    "        else:\n",
    "            raise Exception(\"filter criteria dataframe should have column_filter\")\n",
    "            \n",
    "    filtered_ais_df = ais_df.join(F.broadcast(df_filter), on=column_filter)\n",
    "\n",
    "    return filtered_ais_df\n",
    "\n",
    "def apply_geo_filter(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    polygon: dict\n",
    ") -> DataFrame:\n",
    "    \"\"\"A wrapper function sedona ST_Within filter\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Spark DataFrame\n",
    "        The dataframe should contain latitude, longitude columns\n",
    "    \n",
    "    polygon: dict\n",
    "        GeoJson representation of polygon.\n",
    "        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Spark dataframe with filter applied\n",
    "    \n",
    "    \"\"\"\n",
    "    # SedonaRegistrator.registerAll(spark)\n",
    "    \n",
    "    schema = StructType([StructField(\"geom\", GeometryType(), False)])\n",
    "    gdf = spark.createDataFrame([[shape(polygon)]], schema)\n",
    "    \n",
    "    gdf.createOrReplaceTempView(\"temp2\")\n",
    "    \n",
    "    #drop rows with missing latitude or longitude\n",
    "    df = df.filter(~df.latitude.isNull() & ~df.longitude.isNull())\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp\")\n",
    "    \n",
    "    df = spark.sql(\"\"\"\n",
    "            select *  \n",
    "            from \n",
    "                ( select *, ST_Point(longitude,latitude) as point, \n",
    "                            (select geom from temp2\n",
    "                            ) as polygon \n",
    "                 from temp \n",
    "                ) \n",
    "            where ST_Within(point, polygon) \n",
    "           \"\"\".format(polygon))\n",
    "    return df.drop('point','polygon')\n",
    "\n",
    "def get_ais(\n",
    "    spark: SparkSession,\n",
    "    start_date: datetime,\n",
    "    end_date: datetime = None,\n",
    "    h3_list: Optional[List[int]] = None,\n",
    "    polygon_hex_df: Optional[DataFrame] = None,\n",
    "    mmsi_list: Optional[List[int]] = None,\n",
    "    message_type: Optional[List[int]] = [1,2,3,4,18,19,27],\n",
    "    columns: Optional[List[str]] = [\"*\"],\n",
    "    polygon: Optional[Dict] = None,\n",
    "    polygon_hex_resolution: Optional[int] = 8\n",
    ") -> DataFrame:\n",
    "    \"\"\"A wrapper function to apply filters on the AIS data.\n",
    "    Note that default parameters for message type are \n",
    "    position message types \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark: SparkSession\n",
    "    \n",
    "    start_date: datetime\n",
    "        the start date filter to apply\n",
    "        \n",
    "    end_date: datetime\n",
    "        the end date filter to apply. To filter a single date, use end_date equal to start_date\n",
    "        \n",
    "    h3_list: list of int, default None\n",
    "        h3 indices must be in int format and must have the same resolution. \n",
    "        if None then it is not applied. \n",
    "        \n",
    "    polygon_hex_df: dataframe, from polygon_to_hex_df function\n",
    "        Dataframe with the following columns (minimum columns to contain) :\n",
    "        - hex_id: the h3 hex ids (64-bit ints)\n",
    "        - polygon_name: the name of the polygon \n",
    "        - hex_resolution: the resolution of the hex (should be the same for all)\n",
    "        The hex_ids should be contained in only one polygon_name, otherwise resulting dataframe\n",
    "        will contain duplicate entries.\n",
    "        \n",
    "    mmsi_list: list of int, default None\n",
    "        the list of mmsi filter to apply. if None, then it is not applied\n",
    "        \n",
    "    message_type: list of int, default [1,2,3,4,18,19,27] <- position messages\n",
    "        the list of message types to retain. if not supplied then the default message type filter is applied\n",
    "        use [\"*\"] to get all message types\n",
    "        \n",
    "    columns: list of str, default [\"*\"]\n",
    "        the list of columns to retain. if not supplied, all columns are returned\n",
    "        \n",
    "    polygon: Optional[Dict] = None\n",
    "        GeoJson representation of polygon. If supplied, then the hex approximation of the polygon\n",
    "        is calculated using poly_container (polygon, hex_resolution, overfill=True). The AIS data \n",
    "        will be filtered according to hexes first  and then according to polygon using \n",
    "        Sedona functions: \n",
    "        \n",
    "        select *,  ST_Point(longitude,latitude) as point, ST_GeomFromGeoJSON('{polygon}') as \n",
    "        polygon from temp where ST_Within(point, polygon)\n",
    "        \n",
    "        \n",
    "    polygon_hex_resolution: int = 8\n",
    "        The resolution of the hexagons to fill the input polygon with. Default is 8, a hex with an avg area of 0.737 sq km. \n",
    "        A polygon with an area of 100 sq. km will contain ~136 resolution 8 hexes. The same 100 sq. km polygon \n",
    "        can be approximated by ~949 hexes using resolution 9. Note that the higher the resolution, the higher \n",
    "        the polygon area covered by the hexes. However, a small increase in resolution dramatically increases\n",
    "        the number of hexes. See https://h3geo.org/docs/core-library/restable/ for a table of hex resolutions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Spark dataframe with the filters applied. \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    If multiple filters are provided, the most restrictive filters are applied. For example, both polygon and h3_list\n",
    "    are provided where h3_list is a list of hexes fully contained within the polygon. The filtered AIS data will only contain \n",
    "    those within the hexes. Data within the polygon but outside the hexes will not be included. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #read ais_df \n",
    "    basepath = \"s3a://ungp-ais-data-historical-backup/exact-earth-data/transformed/prod/\"\n",
    "    if end_date is None:\n",
    "        end_date = start_date\n",
    "    dates = pd.date_range(start_date, end_date)\n",
    "    paths = [basepath+\"year=\" +str(x.year) + \"/month=\"+ str(x.month).zfill(2) + \"/day=\"+ str(x.day).zfill(2) for x in dates]\n",
    "    \n",
    "    filtered_ais_df = spark.read.parquet(*paths)\n",
    "        \n",
    "    #Apply mmsi filter\n",
    "    if mmsi_list is not None:\n",
    "        filtered_ais_df = apply_small_filter(spark,\n",
    "            filtered_ais_df,\n",
    "            mmsi_list,\n",
    "            \"mmsi\"\n",
    "        )\n",
    "        \n",
    "    #apply message type filter\n",
    "    if message_type is not [\"*\"]:\n",
    "        filtered_ais_df = apply_small_filter(spark,\n",
    "                filtered_ais_df,\n",
    "                message_type,\n",
    "                \"message_type\"\n",
    "            )\n",
    "    \n",
    "    #apply h3 filter\n",
    "    if h3_list is not None:\n",
    "        unique_h3_res = list(set([h3int.h3_get_resolution(x) for x in h3_list]))\n",
    "        if len(unique_h3_res) != 1:\n",
    "            raise Exception(\"h3_list should contain h3 indices with the same resolution.\")\n",
    "        \n",
    "        filtered_ais_df = apply_small_filter(spark,\n",
    "            filtered_ais_df,\n",
    "            h3_list,\n",
    "            f\"H3_int_index_{unique_h3_res[0]}\"\n",
    "        )\n",
    "    \n",
    "    #apply polygon_hex_df filter\n",
    "    if polygon_hex_df is not None:\n",
    "        if not set([\"hex_id\",\"polygon_name\",\"hex_resolution\"]) <= set(polygon_hex_df.columns):\n",
    "            raise Exception(\"polygon_hex_df should contain columns 'hex_id','polygon_name','hex_resolution'\")\n",
    "          \n",
    "        \n",
    "        column_filter = f\"H3_int_index_{polygon_hex_df.hex_resolution.mode()[0]}\"\n",
    "        poly_copy = polygon_hex_df.rename(columns={'hex_id':column_filter})\n",
    "        \n",
    "        filtered_ais_df = apply_small_filter(spark,\n",
    "            filtered_ais_df,\n",
    "            poly_copy,\n",
    "            column_filter\n",
    "        )\n",
    "        \n",
    "        if columns != [\"*\"]:\n",
    "            columns = list(set(columns + poly_copy.columns.tolist()))\n",
    "    \n",
    "    #apply polygon filter\n",
    "    if polygon is not None:\n",
    "        h3_list = poly_container(polygon, hex_resolution=polygon_hex_resolution, overfill=True)\n",
    "        \n",
    "        filtered_ais_df = apply_small_filter(spark,\n",
    "            filtered_ais_df,\n",
    "            h3_list,\n",
    "            f\"H3_int_index_{polygon_hex_resolution}\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        filtered_ais_df = apply_geo_filter(spark,\n",
    "            filtered_ais_df,\n",
    "            polygon\n",
    "        )\n",
    "\n",
    "    #apply column filter\n",
    "    filtered_ais_df = filtered_ais_df.select(columns)\n",
    "    \n",
    "    return filtered_ais_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3d092-711d-4783-a693-ccface69ed04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a0261-b2f3-42a4-8c0f-192271598512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
